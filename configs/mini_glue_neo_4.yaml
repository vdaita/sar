mode: "glue"
vocab_size: 10001
checkpoint_dir: "./checkpoints"

max_position_slots: 512
max_length: 256
batch_size: 128 
num_train_steps: 32000
num_warmup_steps: 1000
eval_every: 500
save_every: 1000

lr: 0.0003
weight_decay_lambda: 0.01

k: 4

eval_num_batches: 32

n_embed: 64
n_layer: 4
n_head: 4

base_model: "gpt-neo"