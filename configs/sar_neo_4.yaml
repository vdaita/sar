mode: "sar"
vocab_size: 10000
checkpoint_dir: "./checkpoints"

max_length: 256
batch_size: 256
num_train_steps: 32000
num_warmup_steps: 1000
eval_every: 500
save_every: 1000

lr: 0.0003
weight_decay_lambda: 0.01

k: 8

eval_num_batches: 32

n_embed: 256
n_layer: 8
n_head: 16

base_model: "gpt-neo"